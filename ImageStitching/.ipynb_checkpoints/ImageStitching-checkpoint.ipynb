{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c335ac",
   "metadata": {},
   "source": [
    "## Import of Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef6245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc94ff",
   "metadata": {},
   "source": [
    "## Reading the two Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4bb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the two images\n",
    "img1=cv2.imread(\"images/set1/img1.png\")\n",
    "img2=cv2.imread(\"images/set1/img2.png\")\n",
    "\n",
    "# converting the images to grayscale\n",
    "img1_gray=cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)\n",
    "img2_gray=cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# cv2.imshow(\"img1_gray\",img1_gray)\n",
    "# cv2.imshow(\"img2_gray\",img2_gray)\n",
    "\n",
    "# cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a936ba",
   "metadata": {},
   "source": [
    "## Detecting distinctive keypoints and descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83dc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiated ORB for 200 points i.e. when the image is being processed the ORB would look out for 200 keypoints and descriptors\n",
    "orb=cv2.ORB_create(nfeatures=2000)\n",
    "\n",
    "#unpacking keypionts and descriptors\n",
    "key1,des1=orb.detectAndCompute(img1,None)\n",
    "key2,des2=orb.detectAndCompute(img2,None)\n",
    "# here key1,key2 are the list of  keypoints and des1,des2 are the list of  descriptors\n",
    "\n",
    "# draw only keypoints location,not size and orientation\n",
    "# cv2.imshow(\"img1\",cv2.drawKeypoints(img1, key1, None, (255, 0, 255)))\n",
    "# cv2.imshow(\"img2\",cv2.drawKeypoints(img2, key2, None, (255, 0, 255)))\n",
    "\n",
    "           \n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4e4f4",
   "metadata": {},
   "source": [
    "## Matching the Keypoints between two Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48df7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BFMatcher object.\n",
    "# It will find all of the matching keypoints on two images\n",
    "bf = cv2.BFMatcher_create(cv2.NORM_HAMMING)\n",
    "\n",
    "# Find matching points\n",
    "matches = bf.knnMatch(des1, des2,k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4765ba",
   "metadata": {},
   "source": [
    "## Match overlapping keypoints with draw_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e809d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_matches(img1, key1, img2, key2, matches):\n",
    "    r, c = img1.shape[:2]\n",
    "    r1, c1 = img2.shape[:2]\n",
    "    \n",
    "    \n",
    "    # Create a blank image with the size of the first image + second image\n",
    "    output_img = np.zeros((max([r, r1]), c+c1, 3), dtype='uint8')\n",
    "    output_img[:r, :c, :] = np.dstack([img1, img1, img1])\n",
    "    output_img[:r1, c:c+c1, :] = np.dstack([img2, img2, img2])\n",
    "    \n",
    "    # Go over all of the matching points and extract them\n",
    "    for match in matches:\n",
    "        img1_idx = match.queryIdx\n",
    "        img2_idx = match.trainIdx\n",
    "        (x1, y1) = key1[img1_idx].pt\n",
    "        (x2, y2) = key2[img2_idx].pt\n",
    "        \n",
    "        # Draw circles on the keypoints\n",
    "        cv2.circle(output_img, (int(x1),int(y1)), 4, (0, 255, 255), 1)\n",
    "        cv2.circle(output_img, (int(x2)+c,int(y2)), 4, (0, 255, 255), 1)\n",
    "        \n",
    "        # Connect the same keypoints\n",
    "        cv2.line(output_img, (int(x1),int(y1)), (int(x2)+c,int(y2)), (0, 255, 255), 1)\n",
    "        \n",
    "    return output_img\n",
    "        \n",
    "        \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1939b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_matches = []\n",
    "for m, n in matches:\n",
    "    all_matches.append(m)\n",
    "    \n",
    "img3 = draw_matches(img1_gray, key1, img2_gray, key2, all_matches[:30])\n",
    "\n",
    "cv2.imshow(\"match\",img3)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f9a4c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the best matches\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.6 * n.distance:\n",
    "        good.append(m)\n",
    "        \n",
    "# cv2.imshow(\"good points img1\",cv2.drawKeypoints(img1, [key1[m.queryIdx] for m in good], None, (255, 0, 255)))\n",
    "cv2.imshow(\"good points img2\",cv2.drawKeypoints(img2, [key2[m.trainIdx] for m in good], None, (255, 0, 255)))\n",
    "\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96f7b8",
   "metadata": {},
   "source": [
    "## Stitching the two images together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be2ba19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warpImages(img1, img2, H):\n",
    "    rows1, cols1 = img1.shape[:2]\n",
    "    rows2, cols2 = img2.shape[:2]\n",
    "    \n",
    "    list_of_points_1 = np.float32([[0,0], [0, rows1],[cols1, rows1], [cols1, 0]]).reshape(-1, 1, 2) \n",
    "    # coordinates of the reference image (img1)\n",
    "    temp_points = np.float32([[0,0], [0,rows2], [cols2,rows2], [cols2,0]]).reshape(-1,1,2)\n",
    "    # coordinates of the second image(img2) that we want to transform\n",
    "    \n",
    "    # When we have established a homography we need to warp perspective\n",
    "    # Change field of view\n",
    "    list_of_points_2 = cv2.perspectiveTransform(temp_points, H)\n",
    "    \n",
    "    list_of_points = np.concatenate((list_of_points_1,list_of_points_2), axis=0)\n",
    "    \n",
    "    # unpacking the boundary x, y co-ordinates of the resultant stitched image\n",
    "    [x_min, y_min] = np.int32(list_of_points.min(axis=0).ravel() - 0.5)\n",
    "    [x_max, y_max] = np.int32(list_of_points.max(axis=0).ravel() + 0.5)\n",
    "    \n",
    "    translation_dist = [-x_min,-y_min]\n",
    "    \n",
    "    H_translation = np.array([[1, 0, translation_dist[0]], [0, 1, translation_dist[1]], [0, 0, 1]])\n",
    "    \n",
    "    output_img = cv2.warpPerspective(img2, H_translation.dot(H), (x_max-x_min, y_max-y_min))\n",
    "    \n",
    "    \n",
    "    output_img[translation_dist[1]:rows1+translation_dist[1], translation_dist[0]:cols1+translation_dist[0]] = img1\n",
    "    \n",
    "    return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "536d47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_MATCH_COUNT = 10\n",
    "\n",
    "if len(good) > MIN_MATCH_COUNT:\n",
    "    # Convert keypoints to an argument for findHomography\n",
    "    src_pts = np.float32([ key1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ key2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    \n",
    "    # Establish a homography\n",
    "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "    \n",
    "    result = warpImages(img2, img1, M)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"img1\",img1)\n",
    "    cv2.imshow(\"img2\",img2)\n",
    "    cv2.imshow(\"result\",result)\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90b763",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25373b17",
   "metadata": {},
   "source": [
    "### Image 1\n",
    "\n",
    "<img src=\"images/stitch_img1.JPG\" />\n",
    "\n",
    "### Image 2\n",
    "\n",
    "<img src=\"images/stitch_img2.JPG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8274f33",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "<img src=\"images/stitch_res.JPG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ec96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
