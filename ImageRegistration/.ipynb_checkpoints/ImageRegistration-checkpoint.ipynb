{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "im2 = cv2.imread('images/reference_img.jpg') #refernce image\n",
    "im1 = cv2.imread('images/dist_img1.jpg') #distorted image\n",
    "\n",
    "\n",
    "img1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY) #converting the images to grayscale for easy processing\n",
    "img2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#initiated ORB for 200 points i.e. when the image is being processed the ORB would look out for 200 keypoints and descriptors\n",
    "orb = cv2.ORB_create(200)\n",
    "\n",
    "\n",
    "kp1, des1 = orb.detectAndCompute(img1, None) #unpacking keypionts and descriptors\n",
    "kp2, des2 = orb.detectAndCompute(img2, None) #unpacking keypionts and descriptors\n",
    "# here kp1,kp2 are the list of  keypoints and des1,des2 are the list of  descriptors\n",
    "\n",
    "\n",
    "matcher=cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING) # create Matcher object\n",
    "# Brute-Force matcher takes the descriptor of one feature in first set and is \n",
    "# matched with all other features in second set using some distance calculation.\n",
    "\n",
    "\n",
    "\n",
    "#match the descriptors\n",
    "matches=matcher.match(des1,des2,None) # Creates a list of all matches, just like keypoints\n",
    "\n",
    "\n",
    "matches= sorted(matches,key =lambda x:x.distance) #sorting out the keypoints.\n",
    "# Sorting the matches in the order of their distance to find the best keypoints. Otherwise we would end up with \n",
    "# a whole bunch of matches that are probably not good.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------Image Registration section start------------------------------------------\n",
    "\n",
    "# Now let us use these key points(kp1,kp2) to register two images where the distorted image would undergo \n",
    "# distortion correction or alignment.\n",
    "# And for this task we will use homography. \n",
    "\n",
    "\n",
    "# creates an array of zeroes for storing the keypoints\n",
    "points1= np.zeros((len(matches),2),dtype=np.float32) # array of size equal to (matches, 2)\n",
    "points2= np.zeros((len(matches),2),dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "#unpacking the keypoints of the respective refernce and distorted images by enumerating through them\n",
    "for i,match in enumerate(matches):\n",
    "    points1[i,:]=kp1[match.queryIdx].pt # gives index of the descriptor in the list of query/test descriptors\n",
    "    points2[i,:]=kp2[match.trainIdx].pt # gives index of the descriptor in the list of train descriptors\n",
    "\n",
    "    \n",
    "    \n",
    "h, mask = cv2.findHomography(points1,points2,cv2.RANSAC)  # h is the homography matrix \n",
    "# Extract location of good matches and for this we will use RANSAC.\n",
    "# seperating the good points from the bad points is the work of RANSAC\n",
    "\n",
    "\n",
    "# getting the height and the width of the image \n",
    "height,width,channels=im2.shape #\"image 2\" is chosen because it is the refernce image\n",
    "\n",
    "\n",
    "#img1Reg=cv2.wrapPerspective(img1, h, (width,height))\n",
    "im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "\n",
    "\n",
    "# -----------------------------------Image Registration section end--------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------Plotting of keypoints and matching the images section starts--------------\n",
    "\n",
    "\n",
    "\n",
    "# draw only keypoints location,not size and orientation\n",
    "img3 = cv2.drawKeypoints(img1, kp1, None, flags=None) #shows only keypoints\n",
    "img4 = cv2.drawKeypoints(img2, kp2, None, flags=None) #shows only keypoints\n",
    "\n",
    "matches = cv2.drawMatches(im1, kp1,im2, kp2, matches[:100],None) # Draw first 100 matches.\n",
    "# Likewise we used \"cv2.drawKeypoints()\" to draw keypoints, \"cv2.drawMatches()\" helps us to draw the matches between the \n",
    "# referebce image and the distorted image\n",
    "\n",
    "\n",
    "# -----------------------------------Plotting of keypoints and matching the images section ends----------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow(\"Distorted Image\", img3)\n",
    "cv2.imshow(\"Reference Image\", img4)\n",
    "cv2.imshow(\"Matches between the two images\", matches)\n",
    "cv2.imshow(\"Registered image\",im1Reg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                       \n",
    "                                                       \n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Image with keypoints\n",
    "\n",
    "<img src=\"images/ref_img.JPG\" />\n",
    "\n",
    "## Distorted Image with keypoints\n",
    "\n",
    "<img src=\"images/distor_img.JPG\" />\n",
    "\n",
    "## Matched keypoints between the two images\n",
    "\n",
    "<img src=\"images/match_img.JPG\" />\n",
    "\n",
    "## Registered Image\n",
    "\n",
    "<img src=\"images/reg_img.JPG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
